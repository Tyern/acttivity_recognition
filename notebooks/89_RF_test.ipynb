{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import tracemalloc \n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/compgan_dataset/train/train_data0.npy', '../data/compgan_dataset/train/train_data1.npy', '../data/compgan_dataset/train/train_data10.npy', '../data/compgan_dataset/train/train_data11.npy', '../data/compgan_dataset/train/train_data12.npy', '../data/compgan_dataset/train/train_data13.npy', '../data/compgan_dataset/train/train_data14.npy', '../data/compgan_dataset/train/train_data15.npy', '../data/compgan_dataset/train/train_data2.npy', '../data/compgan_dataset/train/train_data3.npy', '../data/compgan_dataset/train/train_data4.npy', '../data/compgan_dataset/train/train_data5.npy', '../data/compgan_dataset/train/train_data6.npy', '../data/compgan_dataset/train/train_data7.npy', '../data/compgan_dataset/train/train_data8.npy', '../data/compgan_dataset/train/train_data9.npy']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"../data/compgan_dataset/\"\n",
    "\n",
    "train_data_file_name_ = \"train_data{}.npy\"\n",
    "train_label_file_name_ = \"train_label{}.npy\"\n",
    "test_data_file_name_ = \"test_data{}.npy\"\n",
    "test_label_file_name_ = \"test_label{}.npy\"\n",
    "\n",
    "TRAIN_FOLDER_PATH = os.path.join(DATA_ROOT, \"train\")\n",
    "TEST_FOLDER_PATH = os.path.join(DATA_ROOT, \"test\")\n",
    "RESULT_FOLDER_PATH = os.path.join(DATA_ROOT, \"results\")\n",
    "PREV_CHECKPOINT_FOLDER_PATH = os.path.join(DATA_ROOT, \"prev_checkpoint\")\n",
    "\n",
    "assert os.path.isdir(TRAIN_FOLDER_PATH) and os.path.isdir(TEST_FOLDER_PATH)\n",
    "os.makedirs(RESULT_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "data_files = sorted(glob.glob(os.path.join(TRAIN_FOLDER_PATH, train_data_file_name_.format(\"*\"))))\n",
    "print(data_files)\n",
    "print(len(data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NUM = 16\n",
    "SENSOR_NUM = 7\n",
    "EACH_SENSOR_CHANNEL = 6\n",
    "assert USER_NUM == len(data_files)\n",
    "feature_num = SENSOR_NUM * EACH_SENSOR_CHANNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "\n",
    "label_list = ['歩行(平地)',\n",
    " '歩行(階段)',\n",
    " 'ベッド上での起き上がり',\n",
    " 'ベッド椅子間の乗り移り(立つ)',\n",
    " 'ベッド椅子間の乗り移り(立たない)',\n",
    " '立ち座り',\n",
    " '座位保持・座位バランス',\n",
    " '立位保持・立位バランス',\n",
    " '関節可動域増大訓練(肩)',\n",
    " '関節可動域増大訓練(股関節)']\n",
    "\n",
    "label_dict = dict(enumerate(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "eng_label_dict = dict(zip(\n",
    "    label_list,\n",
    "    ['Walking', 'Upstair', 'Bed_Standup', 'Change_Bed', 'Change_Bed_Standup', 'Sit_Down', 'Sit', 'Stand', 'Shoulder_Exercise', 'Hip_Exercise']\n",
    "))\n",
    "\n",
    "eng_label_list = [eng_label_dict[i] for i in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update after 2023/11/11 houkoku\n",
    "class CustomTrainDataset(Dataset):\n",
    "    TRAIN_MODE = \"train\"\n",
    "    TEST_MODE = \"test\"\n",
    "    \n",
    "    def __init__(self, mode, feature_data, label_data, missing_sensor_numbers=0):\n",
    "        self.mode = mode\n",
    "        assert mode in [self.TRAIN_MODE, self.TEST_MODE]\n",
    "        \n",
    "        self.features = feature_data\n",
    "        self.label = label_data\n",
    "        assert len(self.features) == len(self.label), \"features len is not equal to label len\"\n",
    "        self.missing_sensor_numbers = missing_sensor_numbers\n",
    "\n",
    "        self.missing_index_list = []\n",
    "        for missing_count in range(missing_sensor_numbers + 1):\n",
    "            for missing_index in combinations(range(SENSOR_NUM), missing_count):\n",
    "                self.missing_index_list.append(missing_index)\n",
    "\n",
    "    def transform(self, one_feature, missing_sensor_id_list):\n",
    "        # Make one sensor data become 0\n",
    "        one_feature_cp = one_feature.copy()\n",
    "        \n",
    "        for missing_sensor_id in missing_sensor_id_list:\n",
    "            one_feature_cp[:, missing_sensor_id*6:(missing_sensor_id+1)*6] = 0\n",
    "        return one_feature_cp\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        # take all available missing pattern * data number\n",
    "        return len(self.features) * len(self.missing_index_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # take all available missing pattern\n",
    "        missing_sensor_id_list = self.missing_index_list[ idx // len(self.features) ]\n",
    "        x = self.transform(self.features[ idx % len(self.features) ], missing_sensor_id_list)\n",
    "        label = self.label[idx % len(self.features)]\n",
    "        return x, int(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    STANDARDIZE = False\n",
    "    \n",
    "    def __init__(self, test_user, missing_sensor_numbers, batch_size=2048):\n",
    "        super().__init__()\n",
    "        self.test_user = test_user\n",
    "        self.missing_sensor_numbers = missing_sensor_numbers\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = None\n",
    "\n",
    "    def load_data(self, mode):\n",
    "        if mode == \"train\":\n",
    "            folder_path = TRAIN_FOLDER_PATH\n",
    "            data_file_name = train_data_file_name_\n",
    "            label_file_name = train_label_file_name_\n",
    "\n",
    "            train_data_file_name = data_file_name.format(self.test_user)\n",
    "            train_label_file_name = label_file_name.format(self.test_user)\n",
    "    \n",
    "            train_data_file_path = os.path.join(folder_path, train_data_file_name)\n",
    "            train_label_file_path = os.path.join(folder_path, train_label_file_name)\n",
    "            train_val_data, train_val_label = np.load(train_data_file_path), np.load(train_label_file_path)\n",
    "            l, s, d, w = train_val_data.shape\n",
    "            self.scaler = StandardScaler()\n",
    "            # train_val_data = self.scaler.fit_transform(train_val_data.reshape(l, -1)).reshape(l, s, d, w)\n",
    "            train_val_data = train_val_data.reshape(l, s ,d, w).transpose(0, 3, 1, 2)\n",
    "            \n",
    "        elif mode == \"test\":\n",
    "            folder_path = TEST_FOLDER_PATH\n",
    "            data_file_name = test_data_file_name_\n",
    "            label_file_name = test_label_file_name_\n",
    "    \n",
    "            train_data_file_name = data_file_name.format(self.test_user)\n",
    "            train_label_file_name = label_file_name.format(self.test_user)\n",
    "    \n",
    "            train_data_file_path = os.path.join(folder_path, train_data_file_name)\n",
    "            train_label_file_path = os.path.join(folder_path, train_label_file_name)\n",
    "            train_val_data, train_val_label = np.load(train_data_file_path), np.load(train_label_file_path)\n",
    "            l, s, d, w = train_val_data.shape\n",
    "            # train_val_data = self.scaler.transform(train_val_data.reshape(l, -1)).reshape(l, s, d, w)\n",
    "            train_val_data = train_val_data.reshape(l, s ,d, w).transpose(0, 3, 1, 2)\n",
    "    \n",
    "        return train_val_data, train_val_label\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign Train/val split(s) for use in Dataloaders\n",
    "        \n",
    "        if stage == \"validate\" or stage == \"fit\":\n",
    "            train_val_data, train_val_label = self.load_data(\"train\")\n",
    "            self.train_data, self.val_data, self.train_label, self.val_label = train_test_split(\n",
    "                train_val_data, train_val_label, test_size=0.2, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "            self.train_dataset = CustomTrainDataset(\n",
    "                CustomTrainDataset.TRAIN_MODE, self.train_data, self.train_label, missing_sensor_numbers=self.missing_sensor_numbers)\n",
    "            self.val_dataset = CustomTrainDataset(\n",
    "                CustomTrainDataset.TEST_MODE, self.val_data, self.val_label, missing_sensor_numbers=self.missing_sensor_numbers)\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            train_val_data, train_val_label = self.load_data(\"test\")\n",
    "            self.test_data = train_val_data\n",
    "            self.test_label = train_val_label\n",
    "\n",
    "            self.test_dataset = CustomTrainDataset(\n",
    "                CustomTrainDataset.TEST_MODE, self.test_data, self.test_label, missing_sensor_numbers=self.missing_sensor_numbers)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=4, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,  num_workers=4, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,  num_workers=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,  num_workers=4, shuffle=False, pin_memory=True)\n",
    "\n",
    "    def teardown(self, stage):\n",
    "        print(\"teardown\")\n",
    "        if stage == \"validate\" or stage == \"fit\":\n",
    "            del self.train_data, self.train_label\n",
    "            del self.val_data, self.val_label\n",
    "            del self.train_dataset\n",
    "            del self.val_dataset\n",
    "            \n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            del self.test_data, self.test_label\n",
    "            del self.test_dataset\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [22:27<00:00, 84.20s/it]\n"
     ]
    }
   ],
   "source": [
    "all_test_pred = {}\n",
    "\n",
    "for missing_sensor_numbers in [0]: ## Changed for 1 missing sensor\n",
    "\n",
    "    all_user_y_true = []\n",
    "    all_user_y_pred = []\n",
    "    \n",
    "    for user in tqdm(range(16)): ## Changed for user 2 only\n",
    "        data_module = DataModule(test_user=user, missing_sensor_numbers=missing_sensor_numbers, batch_size=512)\n",
    "        \n",
    "        train_data, train_label = data_module.load_data(\"train\")\n",
    "        train_data_reshape = train_data.reshape(train_data.shape[0], -1)\n",
    "\n",
    "        classifier = RandomForestClassifier(n_estimators=200, criterion=\"gini\")\n",
    "        classifier.fit(train_data_reshape, train_label)\n",
    "\n",
    "        test_data, test_label = data_module.load_data(\"test\")\n",
    "        test_data_reshape = test_data.reshape(test_data.shape[0], -1)\n",
    "\n",
    "        test_pred = classifier.predict(test_data_reshape)\n",
    "        \n",
    "        all_user_y_true.extend(test_label)\n",
    "        all_user_y_pred.extend(test_pred)\n",
    "        \n",
    "    all_test_pred[missing_sensor_numbers] = (all_user_y_true, all_user_y_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_y_true, all_user_y_pred = all_test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33536121673003805\n"
     ]
    }
   ],
   "source": [
    "# Accuracy:\n",
    "print(accuracy_score(all_user_y_true, all_user_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32099072209201396\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(all_user_y_true, all_user_y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3f15c77c882d8433ef1043b53db379ca3772938d44c39305f8b807ce25a312a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.17 ('compgan': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
