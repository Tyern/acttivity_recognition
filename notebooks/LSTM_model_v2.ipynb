{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03f55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ac51e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587973b",
   "metadata": {},
   "source": [
    "## Read Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8090c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"../data/jikken2/\"\n",
    "\n",
    "feature_save_file = os.path.join(save_folder, \"features.npy\")\n",
    "label_save_file = os.path.join(save_folder, \"label.npy\")\n",
    "label_name_save_file = os.path.join(save_folder, \"label_name.json\")\n",
    "\n",
    "kfold_split_save_file = os.path.join(save_folder, \"kfold_train_val_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89fd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(feature_save_file, allow_pickle=True).astype(np.float)\n",
    "labels = np.load(label_save_file, allow_pickle=True)\n",
    "\n",
    "kfold_train_test_index_list = np.load(kfold_split_save_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba9aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize over the feature data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "data_num, window_size, feature_num = features.shape\n",
    "features_reshape = features.reshape(-1, feature_num)\n",
    "features_norm = sc.fit_transform(features_reshape)\n",
    "\n",
    "# convert back t feature size\n",
    "features = features_norm.reshape(data_num, window_size, feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80510ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_name_save_file) as f:\n",
    "    label_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f310403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698, 200, 30)\n",
      "(698,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0d77a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': '歩いている', '1': '立っている', '2': '走っている', '3': '階段降り', '4': '階段上り', '5': '座っている'}\n"
     ]
    }
   ],
   "source": [
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd62e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, all_feature_list, y_list, data_index_list):\n",
    "\n",
    "        self.all_feature_list = all_feature_list\n",
    "        self.y_list = y_list\n",
    "        self.data_index_list = data_index_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_index_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        index = self.data_index_list[idx]\n",
    "        x = self.all_feature_list[index]\n",
    "        label = self.y_list[index]\n",
    "        \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7fe497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 3.4923e-01, -1.6758e-01, -2.0228e-01,  ...,  1.0886e-01,\n",
       "           -4.8657e-01,  1.5816e-01],\n",
       "          [ 2.9359e-01, -1.7167e-01, -2.4399e-01,  ...,  1.2346e-01,\n",
       "           -4.6355e-01,  1.8221e-01],\n",
       "          [ 2.1474e-01, -1.7858e-01, -2.6724e-01,  ...,  1.5482e-01,\n",
       "           -4.5070e-01,  1.2068e-01],\n",
       "          ...,\n",
       "          [ 9.6204e-02, -3.5357e-01, -1.9214e+00,  ...,  2.7300e+00,\n",
       "           -4.3411e-01, -6.8709e-01],\n",
       "          [ 1.4840e-01, -3.4384e-01, -1.9180e+00,  ...,  2.7316e+00,\n",
       "           -4.6355e-01, -7.0443e-01],\n",
       "          [ 1.6961e-01, -3.2734e-01, -1.8561e+00,  ...,  2.7365e+00,\n",
       "           -4.1805e-01, -7.4191e-01]],\n",
       " \n",
       "         [[-9.1191e-01, -3.7966e-01,  2.8870e-01,  ..., -6.6247e-01,\n",
       "            6.1048e-02,  2.3648e-01],\n",
       "          [-1.1127e+00, -3.9757e-01,  4.3572e-01,  ..., -6.6247e-01,\n",
       "           -5.1239e-04,  5.9113e-01],\n",
       "          [-1.3340e+00, -4.2168e-01,  4.6717e-01,  ..., -5.8677e-01,\n",
       "           -5.2437e-02,  1.0247e+00],\n",
       "          ...,\n",
       "          [-3.8973e-01, -4.4170e-01, -1.0417e+00,  ...,  3.8895e-01,\n",
       "            1.4937e-01, -5.1835e-03],\n",
       "          [-3.9589e-01, -4.3338e-01, -9.9482e-01,  ...,  3.7246e-01,\n",
       "            1.0708e-01, -4.2473e-01],\n",
       "          [-4.1619e-01, -4.2309e-01, -9.8969e-01,  ...,  4.3518e-01,\n",
       "            1.2314e-01, -6.8709e-01]],\n",
       " \n",
       "         [[-1.3199e+00, -1.0145e-01, -9.2815e-01,  ...,  7.1011e-02,\n",
       "           -2.6742e-02,  4.2779e-01],\n",
       "          [-1.3322e+00, -1.1047e-01, -1.0164e+00,  ...,  1.5996e-01,\n",
       "            2.6994e-03,  7.9844e-02],\n",
       "          [-1.3181e+00, -1.1315e-01, -1.0598e+00,  ...,  1.9105e-01,\n",
       "            4.8200e-02, -3.2907e-01],\n",
       "          ...,\n",
       "          [-4.1184e-01, -1.9381e-01,  4.1896e-01,  ...,  5.2575e-01,\n",
       "           -1.0545e+00,  1.8221e-01],\n",
       "          [-4.1891e-01, -1.7858e-01,  4.1725e-01,  ...,  5.2413e-01,\n",
       "           -9.3676e-01,  2.8067e-01],\n",
       "          [-4.1365e-01, -1.5997e-01,  3.5879e-01,  ...,  6.5931e-01,\n",
       "           -6.8570e-01,  3.8639e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 6.7059e-01, -1.0356e-01,  2.0034e+00,  ...,  3.1539e-02,\n",
       "           -4.2802e-02, -1.5253e-02],\n",
       "          [ 6.8291e-01, -1.0427e-01,  2.0116e+00,  ...,  3.1539e-02,\n",
       "           -4.9225e-02, -8.5399e-03],\n",
       "          [ 6.8563e-01, -1.0906e-01,  2.0451e+00,  ...,  2.8294e-02,\n",
       "           -4.2802e-02, -1.5253e-02],\n",
       "          ...,\n",
       "          [ 6.7947e-01, -6.7746e-02,  2.0047e+00,  ...,  3.4783e-02,\n",
       "           -3.3166e-02,  1.5292e-03],\n",
       "          [ 6.8563e-01, -6.5772e-02,  2.0047e+00,  ...,  2.9917e-02,\n",
       "           -2.6742e-02, -8.5399e-03],\n",
       "          [ 6.9089e-01, -6.7041e-02,  1.9931e+00,  ...,  1.9913e-02,\n",
       "           -5.1239e-04, -1.8271e-03]],\n",
       " \n",
       "         [[ 2.6812e+00, -5.6692e-01, -3.5922e-01,  ...,  1.2925e+00,\n",
       "            1.9089e+00,  1.8221e-01],\n",
       "          [ 2.7174e+00, -6.5434e-01, -4.6589e-01,  ...,  1.1670e+00,\n",
       "            1.9153e+00, -1.5510e-01],\n",
       "          [ 2.7740e+00, -7.6306e-01, -6.1633e-01,  ...,  1.1441e+00,\n",
       "            1.4159e+00, -5.0640e-01],\n",
       "          ...,\n",
       "          [ 3.8475e-01, -1.0881e+00,  4.9142e-03,  ..., -1.4784e+00,\n",
       "            9.8820e-01, -1.3863e+00],\n",
       "          [ 3.4307e-01, -1.0370e+00, -1.3549e-02,  ..., -1.2643e+00,\n",
       "            5.0856e-01, -6.2947e-01],\n",
       "          [ 3.5992e-01, -8.3878e-01, -5.6971e-02,  ..., -9.8716e-01,\n",
       "            1.5901e-01,  5.2434e-02]],\n",
       " \n",
       "         [[ 8.3697e-02, -4.5890e-01, -3.4075e-01,  ...,  7.5177e-01,\n",
       "           -5.3528e-01,  1.1270e+00],\n",
       "          [-1.1260e-01, -4.3056e-01, -3.2913e-01,  ...,  1.1852e+00,\n",
       "           -6.8496e-02,  1.2053e+00],\n",
       "          [-3.7015e-01, -3.9616e-01, -3.2229e-01,  ...,  1.5331e+00,\n",
       "            5.0535e-01,  1.0549e+00],\n",
       "          ...,\n",
       "          [ 5.2885e-02, -8.0029e-01, -2.0228e-01,  ..., -1.6631e+00,\n",
       "           -1.4919e+00,  1.6383e+00],\n",
       "          [ 1.2810e-01, -7.6574e-01, -2.5049e-01,  ..., -1.6614e+00,\n",
       "           -1.5310e+00,  1.5393e+00],\n",
       "          [ 2.0858e-01, -7.1695e-01, -2.3545e-01,  ..., -1.6566e+00,\n",
       "           -1.6225e+00,  1.4269e+00]]], dtype=torch.float64),\n",
       " tensor([4, 0, 0, 3, 3, 5, 2, 3])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "# Create train dataset and test dataset for the first activity in label_list\n",
    "train_data_df_index_list, val_data_df_index_list, test_data_df_index_list = kfold_train_test_index_list[0]\n",
    "\n",
    "train_dataset = CustomDataset(features, labels, train_data_df_index_list)\n",
    "val_dataset = CustomDataset(features, labels, val_data_df_index_list)\n",
    "test_dataset = CustomDataset(features, labels, test_data_df_index_list)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=False)\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb582ed",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e58edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, input_size=30, output_size=6):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation, _ = self.rnn(x)\n",
    "        \n",
    "        b, _, _ = activation.size()\n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676fa569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LSTMModel                                --\n",
       "├─LSTM: 1-1                              214,016\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Linear: 2-1                       16,512\n",
       "│    └─BatchNorm1d: 2-2                  256\n",
       "│    └─Dropout1d: 2-3                    --\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       16,512\n",
       "│    └─BatchNorm1d: 2-6                  256\n",
       "│    └─Dropout1d: 2-7                    --\n",
       "│    └─ReLU: 2-8                         --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-9                       16,512\n",
       "│    └─BatchNorm1d: 2-10                 256\n",
       "│    └─Dropout1d: 2-11                   --\n",
       "│    └─ReLU: 2-12                        --\n",
       "│    └─Linear: 2-13                      16,512\n",
       "│    └─BatchNorm1d: 2-14                 256\n",
       "│    └─Dropout1d: 2-15                   --\n",
       "│    └─ReLU: 2-16                        --\n",
       "├─Linear: 1-4                            2,310\n",
       "=================================================================\n",
       "Total params: 283,398\n",
       "Trainable params: 283,398\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9d85c",
   "metadata": {},
   "source": [
    "## Train step setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6357a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting early_stopping_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile early_stopping_utils.py\n",
    "\n",
    "# Inspired from https://github.com/Bjarten/early-stopping-pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.trace_func(f'EarlyStopping patience reached')\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "696b5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.float().to(device), y.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35b247b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "          dataloader: torch.utils.data.DataLoader):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "\n",
    "    return test_pred_labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca20a156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************KFOLD 1*************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a1170f97c24d6b8ed2fc9b69730e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/tranhoang/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/tranhoang/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'CustomDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 51\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m val_step(\n\u001b[1;32m     57\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[1;32m     59\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Append train loss and val loss for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [15], line 12\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Loop through data loader data batches\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Send data to target device\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1027\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# import EarlyStopping\n",
    "from early_stopping_utils import EarlyStopping\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 1\n",
    "\n",
    "patience = 20\n",
    "best_pt = \"weights/best.pt\"\n",
    "\n",
    "all_test = []\n",
    "all_pred = []\n",
    "loss_all_folds = []\n",
    "\n",
    "for i, (train_index, val_index, test_index) in enumerate(kfold_train_test_index_list):\n",
    "    print(f\"\\n*************KFOLD {i + 1}*************\")\n",
    "    \n",
    "    one_fold_loss = []\n",
    "\n",
    "    train_dataset = CustomDataset(features, labels, train_index)\n",
    "    val_dataset = CustomDataset(features, labels, val_index)\n",
    "    test_dataset = CustomDataset(features, labels, test_index)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=4, # number of subprocesses to use for data loading\n",
    "        shuffle=True)\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=2, # number of subprocesses to use for data loading\n",
    "        shuffle=False)\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2, # number of subprocesses to use for data loading\n",
    "        shuffle=False)\n",
    "    \n",
    "    model = LSTMModel(hidden_size=64, input_size=feature_num, output_size=len(label_list)).to(device)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=False, path=best_pt)\n",
    "\n",
    "    # Setup loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0005)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        \n",
    "        val_loss, val_acc = val_step(\n",
    "            model=model,\n",
    "            dataloader=val_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # Append train loss and val loss for plotting\n",
    "        one_fold_loss.append([train_loss, val_loss])\n",
    "        \n",
    "#         print(\n",
    "#             f\"Epoch: {epoch+1} | \"\n",
    "#             f\"train_loss: {train_loss:.4f} | \"\n",
    "#             f\"train_acc: {train_acc:.4f} | \"\n",
    "#             f\"val_loss: {test_loss:.4f} | \"\n",
    "#             f\"val_acc: {test_acc:.4f}\"\n",
    "#         )\n",
    "        \n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch: {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(best_pt))\n",
    "    \n",
    "    y_pred, y_true = test_step(model, test_dataloader)\n",
    "\n",
    "    all_test.extend(y_true.cpu().numpy())\n",
    "    all_pred.extend(y_pred.cpu().numpy())\n",
    "    \n",
    "    loss_all_folds.append(one_fold_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90842db6",
   "metadata": {},
   "source": [
    "## Visualize the training process loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will visualize loss graph for fold k\n",
    "\n",
    "def visualize_loss_graph(one_fold_data,  title=\"Loss graph\"):\n",
    "    train_loss_list = list(map(lambda x: x[0], one_fold_data))\n",
    "    val_loss_list = list(map(lambda x: x[1], one_fold_data))\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.plot(range(1,len(one_fold_data)+1),train_loss_list, label='Training Loss')\n",
    "    plt.plot(range(1,len(one_fold_data)+1),val_loss_list,label='Validation Loss')\n",
    "    plt.plot(np.argmin(val_loss_list) + 1, np.min(val_loss_list), 'ro', label='Stop point')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d45b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_fold_data = loss_all_folds[4]\n",
    "visualize_loss_graph(one_fold_data, title=\"Loss graph for first fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccd5ce",
   "metadata": {},
   "source": [
    "## Evaluation and metric testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_with_label = [label_list[i] for i in all_test]\n",
    "all_pred_with_label = [label_list[i] for i in all_pred]\n",
    "\n",
    "cf = confusion_matrix(all_test_with_label, all_pred_with_label, labels=label_list)\n",
    "sns.heatmap(cf, annot=True, xticklabels=eng_label_list, yticklabels=eng_label_list, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e98ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision_recall_fscore_support: \")\n",
    "print()\n",
    "print(*eng_label_list, sep=\" \"*4)\n",
    "print(*precision_recall_fscore_support(all_test_with_label, all_pred_with_label, labels=label_list), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ccf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
