{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03f55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ac51e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587973b",
   "metadata": {},
   "source": [
    "## Read Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8090c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"../data/jikken2/\"\n",
    "\n",
    "feature_save_file = os.path.join(save_folder, \"features.npy\")\n",
    "label_save_file = os.path.join(save_folder, \"label.npy\")\n",
    "label_name_save_file = os.path.join(save_folder, \"label_name.json\")\n",
    "\n",
    "kfold_split_save_file = os.path.join(save_folder, \"kfold_train_val_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89fd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(feature_save_file, allow_pickle=True).astype(np.float)\n",
    "labels = np.load(label_save_file, allow_pickle=True)\n",
    "\n",
    "kfold_train_test_index_list = np.load(kfold_split_save_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba9aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize over the feature data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "data_num, window_size, feature_num = features.shape\n",
    "features_reshape = features.reshape(-1, feature_num)\n",
    "features_norm = sc.fit_transform(features_reshape)\n",
    "\n",
    "# convert back t feature size\n",
    "features = features_norm.reshape(data_num, window_size, feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80510ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_name_save_file) as f:\n",
    "    label_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f310403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(698, 200, 30)\n",
      "(698,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0d77a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': '歩いている', '1': '立っている', '2': '走っている', '3': '階段降り', '4': '階段上り', '5': '座っている'}\n"
     ]
    }
   ],
   "source": [
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd62e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, all_feature_list, y_list, data_index_list):\n",
    "\n",
    "        self.all_feature_list = all_feature_list\n",
    "        self.y_list = y_list\n",
    "        self.data_index_list = data_index_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_index_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        index = self.data_index_list[idx]\n",
    "        x = self.all_feature_list[index]\n",
    "        label = self.y_list[index]\n",
    "        \n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7fe497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-4.0024e-01, -8.8771e-01, -9.1140e-01,  ..., -7.5493e-01,\n",
       "            1.6311e+00,  2.0786e+00],\n",
       "          [-4.8090e-01, -1.0040e+00, -9.9311e-01,  ..., -9.7230e-01,\n",
       "            1.5203e+00,  1.7306e+00],\n",
       "          [-4.4193e-01, -1.0901e+00, -1.0731e+00,  ..., -1.1505e+00,\n",
       "            1.5171e+00,  1.2255e+00],\n",
       "          ...,\n",
       "          [ 3.3785e+00,  3.0739e+00, -1.1569e+00,  ...,  8.0449e-01,\n",
       "           -1.2306e+00, -1.6517e-01],\n",
       "          [ 3.1918e+00,  4.1102e+00, -1.3788e+00,  ...,  2.5702e-01,\n",
       "           -2.0759e+00, -5.2374e-01],\n",
       "          [ 2.6431e+00,  4.8215e+00, -1.5757e+00,  ..., -1.1013e-01,\n",
       "           -2.7584e+00, -9.7406e-01]],\n",
       " \n",
       "         [[-2.8352e-01, -2.5853e-01, -3.1887e-01,  ...,  5.8034e-02,\n",
       "            1.8759e-02, -4.6019e-02],\n",
       "          [-2.7898e-01, -2.5233e-01, -3.3084e-01,  ...,  4.4786e-02,\n",
       "            2.5182e-02, -4.9376e-02],\n",
       "          [-2.8170e-01, -2.5642e-01, -3.2742e-01,  ...,  4.1542e-02,\n",
       "            4.4989e-02, -2.9237e-02],\n",
       "          ...,\n",
       "          [-2.6485e-01, -2.2539e-01, -3.3221e-01,  ...,  2.5050e-02,\n",
       "            5.4624e-02, -1.5253e-02],\n",
       "          [-2.6485e-01, -2.2342e-01, -3.2058e-01,  ...,  3.1539e-02,\n",
       "            5.4624e-02, -2.1965e-02],\n",
       "          [-2.6050e-01, -2.2539e-01, -3.3221e-01,  ...,  2.8294e-02,\n",
       "            4.4989e-02, -1.5253e-02]],\n",
       " \n",
       "         [[ 5.4661e-01, -1.2485e-01, -1.8895e-01,  ..., -5.1918e-01,\n",
       "           -6.8892e-01, -4.6221e-01],\n",
       "          [ 5.4842e-01, -1.2485e-01, -1.4860e-01,  ..., -5.2404e-01,\n",
       "           -4.8978e-01, -4.4487e-01],\n",
       "          [ 5.3066e-01, -1.2218e-01, -9.5265e-02,  ..., -5.7676e-01,\n",
       "           -1.0683e-02, -3.6320e-01],\n",
       "          ...,\n",
       "          [-1.1711e+00, -3.3214e-01, -5.0282e-01,  ..., -1.7817e+00,\n",
       "           -2.0232e-01, -4.1802e-01],\n",
       "          [-7.6492e-01, -2.3992e-01, -3.6229e-01,  ..., -1.7769e+00,\n",
       "           -6.3003e-01, -2.8824e-01],\n",
       "          [-3.7559e-01, -1.6828e-01, -2.1733e-01,  ..., -1.7769e+00,\n",
       "           -1.0090e+00, -7.3430e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-4.7292e-01, -4.1406e-01, -7.3724e-02,  ..., -2.7180e-01,\n",
       "            1.8923e+00, -3.2899e+00],\n",
       "          [-5.6318e-01, -4.2929e-01, -1.2535e-01,  ...,  5.6141e-02,\n",
       "            2.4148e+00, -3.6547e+00],\n",
       "          [-7.1634e-01, -4.3606e-01, -2.5562e-01,  ...,  4.8952e-01,\n",
       "            2.4539e+00, -3.2967e+00],\n",
       "          ...,\n",
       "          [-3.2339e-01, -3.6316e-01,  4.9726e-01,  ...,  8.5883e-01,\n",
       "           -2.6121e-01,  5.4358e-01],\n",
       "          [-3.5873e-01, -3.3834e-01,  4.4734e-01,  ...,  8.4234e-01,\n",
       "           -3.5596e-01,  1.8893e-01],\n",
       "          [-3.7650e-01, -2.9562e-01,  4.3708e-01,  ...,  6.3308e-01,\n",
       "           -3.6238e-01, -2.0657e-01]],\n",
       " \n",
       "         [[ 1.6073e-01,  2.4543e-01, -1.3870e+00,  ..., -1.7653e+00,\n",
       "            1.6054e+00, -7.1450e-01],\n",
       "          [ 1.5185e-01, -1.8252e-02, -1.3955e+00,  ..., -1.6136e+00,\n",
       "            2.1145e+00, -6.5688e-01],\n",
       "          [ 1.6164e-01, -2.3583e-01, -1.4003e+00,  ..., -1.6747e+00,\n",
       "            2.7670e+00, -5.0976e-01],\n",
       "          ...,\n",
       "          [ 3.0635e+00, -7.1554e-01, -1.0099e+00,  ...,  1.6337e+00,\n",
       "            9.8499e-01, -1.1033e+00],\n",
       "          [ 3.0644e+00, -7.1159e-01, -9.8149e-01,  ...,  1.7456e+00,\n",
       "            2.6714e-01, -9.3658e-01],\n",
       "          [ 2.9484e+00, -6.9298e-01, -8.6456e-01,  ...,  2.3393e+00,\n",
       "           -7.3442e-01, -4.8962e-01]],\n",
       " \n",
       "         [[ 7.3874e-01,  6.8610e-02,  1.8314e+00,  ...,  2.6672e-02,\n",
       "           -7.4714e-03, -1.5253e-02],\n",
       "          [ 7.2714e-01,  6.9879e-02,  1.8396e+00,  ...,  3.3161e-02,\n",
       "           -1.3895e-02, -1.1896e-02],\n",
       "          [ 7.4055e-01,  7.6083e-02,  1.8331e+00,  ...,  3.3161e-02,\n",
       "           -7.4714e-03, -8.5399e-03],\n",
       "          ...,\n",
       "          [ 7.5197e-01,  7.4109e-02,  1.8430e+00,  ...,  2.9917e-02,\n",
       "           -2.3531e-02, -5.1835e-03],\n",
       "          [ 7.4762e-01,  7.2699e-02,  1.8464e+00,  ...,  3.3161e-02,\n",
       "           -2.0319e-02, -5.1835e-03],\n",
       "          [ 7.4762e-01,  7.2699e-02,  1.8345e+00,  ...,  3.4783e-02,\n",
       "           -1.7107e-02, -1.8271e-03]]], dtype=torch.float64),\n",
       " tensor([2, 1, 3, 3, 3, 4, 2, 5])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "# Create train dataset and test dataset for the first activity in label_list\n",
    "train_data_df_index_list, val_data_df_index_list, test_data_df_index_list = kfold_train_test_index_list[0]\n",
    "\n",
    "train_dataset = CustomDataset(features, labels, train_data_df_index_list)\n",
    "val_dataset = CustomDataset(features, labels, val_data_df_index_list)\n",
    "test_dataset = CustomDataset(features, labels, test_data_df_index_list)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0, # number of subprocesses to use for data loading\n",
    "    shuffle=False)\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb582ed",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e58edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, input_size=30, output_size=6):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation, _ = self.rnn(x)\n",
    "        \n",
    "        b, _, _ = activation.size()\n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676fa569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LSTMModel                                --\n",
       "├─LSTM: 1-1                              214,016\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Linear: 2-1                       16,512\n",
       "│    └─BatchNorm1d: 2-2                  256\n",
       "│    └─Dropout1d: 2-3                    --\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       16,512\n",
       "│    └─BatchNorm1d: 2-6                  256\n",
       "│    └─Dropout1d: 2-7                    --\n",
       "│    └─ReLU: 2-8                         --\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-9                       16,512\n",
       "│    └─BatchNorm1d: 2-10                 256\n",
       "│    └─Dropout1d: 2-11                   --\n",
       "│    └─ReLU: 2-12                        --\n",
       "│    └─Linear: 2-13                      16,512\n",
       "│    └─BatchNorm1d: 2-14                 256\n",
       "│    └─Dropout1d: 2-15                   --\n",
       "│    └─ReLU: 2-16                        --\n",
       "├─Linear: 1-4                            2,310\n",
       "=================================================================\n",
       "Total params: 283,398\n",
       "Trainable params: 283,398\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9d85c",
   "metadata": {},
   "source": [
    "## Train step setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6357a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting early_stopping_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile early_stopping_utils.py\n",
    "\n",
    "# Inspired from https://github.com/Bjarten/early-stopping-pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.trace_func(f'EarlyStopping patience reached')\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696b5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.float().to(device), y.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b247b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "          dataloader: torch.utils.data.DataLoader):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "\n",
    "    return test_pred_labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca20a156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*************KFOLD 1*************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd29283585f34845a6868353ff75e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 51\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m val_step(\n\u001b[1;32m     57\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[1;32m     59\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Append train loss and val loss for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [14], line 27\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 4. Loss backward\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 5. Optimizer step\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gan/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# import EarlyStopping\n",
    "from early_stopping_utils import EarlyStopping\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "\n",
    "patience = 20\n",
    "best_pt = \"weights/best.pt\"\n",
    "\n",
    "all_test = []\n",
    "all_pred = []\n",
    "loss_all_folds = []\n",
    "\n",
    "for i, (train_index, val_index, test_index) in enumerate(kfold_train_test_index_list):\n",
    "    print(f\"\\n*************KFOLD {i + 1}*************\")\n",
    "    \n",
    "    one_fold_loss = []\n",
    "\n",
    "    train_dataset = CustomDataset(features, labels, train_index)\n",
    "    val_dataset = CustomDataset(features, labels, val_index)\n",
    "    test_dataset = CustomDataset(features, labels, test_index)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=4, # number of subprocesses to use for data loading\n",
    "        shuffle=True)\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=2, # number of subprocesses to use for data loading\n",
    "        shuffle=False)\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2, # number of subprocesses to use for data loading\n",
    "        shuffle=False)\n",
    "    \n",
    "    model = LSTMModel(hidden_size=64, input_size=feature_num, output_size=len(label_list)).to(device)\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=False, path=best_pt)\n",
    "\n",
    "    # Setup loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0005)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        \n",
    "        val_loss, val_acc = val_step(\n",
    "            model=model,\n",
    "            dataloader=val_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # Append train loss and val loss for plotting\n",
    "        one_fold_loss.append([train_loss, val_loss])\n",
    "        \n",
    "#         print(\n",
    "#             f\"Epoch: {epoch+1} | \"\n",
    "#             f\"train_loss: {train_loss:.4f} | \"\n",
    "#             f\"train_acc: {train_acc:.4f} | \"\n",
    "#             f\"val_loss: {test_loss:.4f} | \"\n",
    "#             f\"val_acc: {test_acc:.4f}\"\n",
    "#         )\n",
    "        \n",
    "        early_stopping(val_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch: {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(best_pt))\n",
    "    \n",
    "    y_pred, y_true = test_step(model, test_dataloader)\n",
    "\n",
    "    all_test.extend(y_true.cpu().numpy())\n",
    "    all_pred.extend(y_pred.cpu().numpy())\n",
    "    \n",
    "    loss_all_folds.append(one_fold_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90842db6",
   "metadata": {},
   "source": [
    "## Visualize the training process loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will visualize loss graph for fold k\n",
    "\n",
    "def visualize_loss_graph(one_fold_data,  title=\"Loss graph\"):\n",
    "    train_loss_list = list(map(lambda x: x[0], one_fold_data))\n",
    "    val_loss_list = list(map(lambda x: x[1], one_fold_data))\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.plot(range(1,len(one_fold_data)+1),train_loss_list, label='Training Loss')\n",
    "    plt.plot(range(1,len(one_fold_data)+1),val_loss_list,label='Validation Loss')\n",
    "    plt.plot(np.argmin(val_loss_list) + 1, np.min(val_loss_list), 'ro', label='Stop point')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d45b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_fold_data = loss_all_folds[4]\n",
    "visualize_loss_graph(one_fold_data, title=\"Loss graph for first fold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccd5ce",
   "metadata": {},
   "source": [
    "## Evaluation and metric testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_with_label = [label_list[i] for i in all_test]\n",
    "all_pred_with_label = [label_list[i] for i in all_pred]\n",
    "\n",
    "cf = confusion_matrix(all_test_with_label, all_pred_with_label, labels=label_list)\n",
    "sns.heatmap(cf, annot=True, xticklabels=eng_label_list, yticklabels=eng_label_list, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e98ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision_recall_fscore_support: \")\n",
    "print()\n",
    "print(*eng_label_list, sep=\" \"*4)\n",
    "print(*precision_recall_fscore_support(all_test_with_label, all_pred_with_label, labels=label_list), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ccf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
