{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce6502-93ec-4660-8864-7ff3019c6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tran/miniconda3/envs/compgan/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb9b14e-4055-471a-9cc3-b4291503788a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2324, 0.6625], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c153c485-8474-4042-a29b-31ea8978c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/compgan_dataset/train/train_data0.npy', '../data/compgan_dataset/train/train_data1.npy', '../data/compgan_dataset/train/train_data10.npy', '../data/compgan_dataset/train/train_data11.npy', '../data/compgan_dataset/train/train_data12.npy', '../data/compgan_dataset/train/train_data13.npy', '../data/compgan_dataset/train/train_data14.npy', '../data/compgan_dataset/train/train_data15.npy', '../data/compgan_dataset/train/train_data2.npy', '../data/compgan_dataset/train/train_data3.npy', '../data/compgan_dataset/train/train_data4.npy', '../data/compgan_dataset/train/train_data5.npy', '../data/compgan_dataset/train/train_data6.npy', '../data/compgan_dataset/train/train_data7.npy', '../data/compgan_dataset/train/train_data8.npy', '../data/compgan_dataset/train/train_data9.npy']\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"../data/compgan_dataset/\"\n",
    "\n",
    "train_data_file_name_ = \"train_data{}.npy\"\n",
    "train_label_file_name_ = \"train_label{}.npy\"\n",
    "test_data_file_name_ = \"test_data{}.npy\"\n",
    "test_label_file_name_ = \"test_label{}.npy\"\n",
    "\n",
    "TRAIN_FOLDER_PATH = os.path.join(DATA_ROOT, \"train\")\n",
    "TEST_FOLDER_PATH = os.path.join(DATA_ROOT, \"test\")\n",
    "RESULT_FOLDER_PATH = os.path.join(DATA_ROOT, \"results\")\n",
    "\n",
    "assert os.path.isdir(TRAIN_FOLDER_PATH) and os.path.isdir(TEST_FOLDER_PATH)\n",
    "os.makedirs(RESULT_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "data_files = sorted(glob.glob(os.path.join(TRAIN_FOLDER_PATH, train_data_file_name_.format(\"*\"))))\n",
    "print(data_files)\n",
    "print(len(data_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878427cd-91fa-4b99-94c8-399378c81dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NUM = 16\n",
    "assert USER_NUM == len(data_files)\n",
    "STANDARDIZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee7a344b-73b9-4310-a650-c06bea9f68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "\n",
    "label_list = ['歩行(平地)',\n",
    " '歩行(階段)',\n",
    " 'ベッド上での起き上がり',\n",
    " 'ベッド椅子間の乗り移り(立つ)',\n",
    " 'ベッド椅子間の乗り移り(立たない)',\n",
    " '立ち座り',\n",
    " '座位保持・座位バランス',\n",
    " '立位保持・立位バランス',\n",
    " '関節可動域増大訓練(肩)',\n",
    " '関節可動域増大訓練(股関節)']\n",
    "\n",
    "label_dict = dict(enumerate(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f45c18-d2c9-46e9-89bf-75b68dbd12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "eng_label_dict = dict(zip(\n",
    "    label_list,\n",
    "    ['Walking', 'Upstair', 'Bed_Standup', 'Change_Bed', 'Change_Bed_Standup', 'Sit_Down', 'Sit', 'Stand', 'Shoulder_Exercise', 'Hip_Exercise']\n",
    "))\n",
    "\n",
    "eng_label_list = [eng_label_dict[i] for i in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b4822e-6d49-41d2-bbd0-42e9ef3e9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029a9b60-c450-419a-8bb7-ba52564721a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(test_user):\n",
    "    train_data_file_name = train_data_file_name_.format(test_user)\n",
    "    train_label_file_name = train_label_file_name_.format(test_user)\n",
    "    test_data_file_name = test_data_file_name_.format(test_user)\n",
    "    test_label_file_name = test_label_file_name_.format(test_user)\n",
    "\n",
    "    train_data_file_path = os.path.join(TRAIN_FOLDER_PATH, train_data_file_name)\n",
    "    train_label_file_path = os.path.join(TRAIN_FOLDER_PATH, train_label_file_name)\n",
    "    test_data_file_path = os.path.join(TEST_FOLDER_PATH, test_data_file_name)\n",
    "    test_label_file_path = os.path.join(TEST_FOLDER_PATH, test_label_file_name)\n",
    "\n",
    "    train_data, train_label = np.load(train_data_file_path), np.load(train_label_file_path)\n",
    "    test_data, test_label = np.load(test_data_file_path), np.load(test_label_file_path)\n",
    "\n",
    "    if STANDARDIZE:\n",
    "        l, s, d, w = train_data.shape\n",
    "        train_data_reshape = train_data.reshape(l, s * d, w).transpose(0,2,1).reshape(-1, s * d)\n",
    "    \n",
    "        l_t, s_t, d_t, w_t = test_data.shape\n",
    "        test_data_reshape = test_data.reshape(l_t, s_t * d_t, w_t).transpose(0,2,1).reshape(-1, s_t * d_t)\n",
    "    \n",
    "        sc = StandardScaler()\n",
    "        train_data_reshape_norm = sc.fit_transform(train_data_reshape)\n",
    "        test_data_reshape_norm = sc.transform(test_data_reshape)\n",
    "    \n",
    "        train_data_reshape_back = train_data_reshape_norm.reshape(l, w, s * d)\n",
    "        test_data_reshape_back = test_data_reshape_norm.reshape(l_t, w_t, s_t * d_t)\n",
    "    else:\n",
    "        l, s, d, w = train_data.shape\n",
    "        train_data_reshape_back = train_data.reshape(l, s * d, w).transpose(0,2,1)\n",
    "\n",
    "        l_t, s_t, d_t, w_t = test_data.shape\n",
    "        test_data_reshape_back = test_data.reshape(l_t, s_t * d_t, w_t).transpose(0,2,1)\n",
    "\n",
    "    return train_data_reshape_back, train_label, test_data_reshape_back, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b906899-e306-4105-a34c-2a9dec2ccbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "train_data, train_label, test_data, test_label = load_train_test(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74804555-d584-43d3-b635-ed3ca4e20381",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = train_data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15253300-be5a-4855-a356-f565676ac443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, feature_data, label_data, missing_sensor_id_list=None):\n",
    "        self.features = feature_data\n",
    "        self.label = label_data\n",
    "        \n",
    "        if missing_sensor_id_list is not None:\n",
    "            for missing_sensor_id in missing_sensor_id_list:\n",
    "                self.features[:, :, missing_sensor_id*6:(missing_sensor_id+1)*6] = 0\n",
    "\n",
    "        assert len(self.features) == len(self.label), \"features len is not equal to label len\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        label = self.label[idx]\n",
    "        return x, int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e461f5f5-df7a-4904-aebc-c33a91b97683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(test_user, missing_sensor_id_list, echo=0):\n",
    "    if echo: \n",
    "        print(\"***********Start get_train_test_dataset***********\")\n",
    "        print(\"missing_sensor_id_list:\", missing_sensor_id_list)\n",
    "    \n",
    "    train_dataset_concat_list = []\n",
    "    for user in range(USER_NUM):\n",
    "        if user == test_user:\n",
    "            if echo: print(\"test_user\", user)\n",
    "            train_data, train_label, test_data, test_label = load_train_test(user)\n",
    "            test_dataset = CustomDataset(test_data, test_label, missing_sensor_id_list=missing_sensor_id_list)\n",
    "    \n",
    "        else:\n",
    "            if echo: print(\"train_user\", user)\n",
    "            train_data, train_label, test_data, test_label = load_train_test(user)\n",
    "            train_dataset = CustomDataset(train_data, train_label, missing_sensor_id_list=missing_sensor_id_list)\n",
    "            train_dataset_concat_list.append(train_dataset)\n",
    "\n",
    "    concat_train_dataset = torch.utils.data.ConcatDataset(train_dataset_concat_list)\n",
    "    if not echo: print(\"***********get_train_test_dataset completed***********\")\n",
    "    return concat_train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62eebcce-61ad-49a9-b1f2-50471ff8a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_missing_sensor_number(test_user, missing_sensor_numbers, echo=0):\n",
    "    if echo: print(\"****************Start create_dataset_for_missing_sensor_number\", test_user, missing_sensor_numbers, \"****************\")\n",
    "    train_dataset_list = []\n",
    "    val_dataset_list = []\n",
    "    test_dataset_list = []\n",
    "    \n",
    "    for missing_count in range(missing_sensor_numbers + 1):\n",
    "        for missing_index in combinations(range(7), missing_count):\n",
    "\n",
    "            dataset, test_dataset = get_train_test_dataset(test_user=test_user, missing_sensor_id_list=missing_index, echo=echo)\n",
    "            train_size = int(0.8 * len(dataset))\n",
    "            val_size = len(dataset) - train_size\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "            train_dataset_list.append(train_dataset)\n",
    "            val_dataset_list.append(val_dataset)\n",
    "            test_dataset_list.append(test_dataset)\n",
    "\n",
    "    del dataset, test_dataset, train_dataset, val_dataset\n",
    "    gc.collect()\n",
    "    \n",
    "    train_dataset = torch.utils.data.ConcatDataset(train_dataset_list)\n",
    "    val_dataset = torch.utils.data.ConcatDataset(val_dataset_list)\n",
    "    test_dataset = torch.utils.data.ConcatDataset(test_dataset_list)\n",
    "    if echo: print(\"****************Completed create_dataset_for_missing_sensor_number****************\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1b720-7426-486d-95f4-aca5fc8368f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccdb0600-ebb2-4d73-94f2-d8e4729336bd",
   "metadata": {},
   "source": [
    "## MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecabfca0-2c1a-4485-974a-1acd2865ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        \n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97b4ec5-f1bf-49ec-bc3d-266a58f4099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, input_size=42, output_size=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(1024, 256, input_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=3,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "\n",
    "        self.all_test = []\n",
    "        self.all_pred = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation, _ = self.rnn(x)\n",
    "        \n",
    "        b, _, _ = activation.size()\n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.0005)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        # 1. Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "    \n",
    "        # 1. Forward pass\n",
    "        test_pred_logits = self.forward(X)\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "        test_acc = ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "\n",
    "        self.all_pred = test_pred_labels\n",
    "        self.all_test = y\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0f1c7eb-386b-4f8a-96fa-30afd4245ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBiModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, input_size=42, output_size=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(1024, 256, input_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=3,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "\n",
    "        double_hidden_size = hidden_size * 2\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=double_hidden_size, out_features=double_hidden_size),\n",
    "            nn.BatchNorm1d(num_features=double_hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=double_hidden_size, out_features=double_hidden_size),\n",
    "            nn.BatchNorm1d(num_features=double_hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=double_hidden_size, out_features=double_hidden_size),\n",
    "            nn.BatchNorm1d(num_features=double_hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=double_hidden_size, out_features=double_hidden_size),\n",
    "            nn.BatchNorm1d(num_features=double_hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * double_hidden_size, out_features=output_size)\n",
    "\n",
    "        self.all_test = []\n",
    "        self.all_pred = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation, (h, _) = self.rnn(x)\n",
    "        b, _, _ = activation.size()\n",
    "        \n",
    "        lstm_output = h[-2:].permute(1,0,2).reshape(b, -1)\n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.0005)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        # 1. Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "    \n",
    "        # 1. Forward pass\n",
    "        test_pred_logits = self.forward(X)\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "        test_acc = ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "\n",
    "        self.all_pred = test_pred_labels\n",
    "        self.all_test = y\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05745f4a-c9d2-44ff-9cc7-ba34e61937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, input_size=42, output_size=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(1024, 256, input_size)\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.attention1 = SelfAttention(\n",
    "            input_dim=hidden_size)\n",
    "\n",
    "        self.rnn2 = nn.LSTM(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.attention2 = SelfAttention(\n",
    "            input_dim=hidden_size\n",
    "        )\n",
    "\n",
    "        self.rnn3 = nn.LSTM(input_size=hidden_size, \n",
    "                  hidden_size=hidden_size,\n",
    "                  num_layers=1,\n",
    "                  batch_first=True)\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "        \n",
    "        self.all_test = []\n",
    "        self.all_pred = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activation, _ = self.rnn1(x)\n",
    "        activation = self.attention1(activation)\n",
    "        activation, _ = self.rnn2(activation)\n",
    "        activation = self.attention2(activation)\n",
    "        activation, (h, _) = self.rnn3(activation)\n",
    "\n",
    "        b, _, _ = activation.size()\n",
    "        \n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        \n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.0005)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        # 1. Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        y = y\n",
    "    \n",
    "        # 1. Forward pass\n",
    "        test_pred_logits = self.forward(X)\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "        test_acc = ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "        \n",
    "        self.all_pred = test_pred_labels\n",
    "        self.all_test = y\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        y = y\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd7f246-f58c-4a18-8195-bead57c332ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, sequence_length=256, input_size=42, cnn_filter_size=64, output_size=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(1024, sequence_length, input_size)\n",
    "        \n",
    "        self.cnn = nn.Conv1d(sequence_length, cnn_filter_size, kernel_size=5, padding=\"same\")\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=3,\n",
    "                          batch_first=True)\n",
    " \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "\n",
    "        self.all_test = []\n",
    "        self.all_pred = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.cnn(x)\n",
    "        activation, _ = self.rnn(output)\n",
    "        \n",
    "        b, _, _ = activation.size()\n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.0005)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        # 1. Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "    \n",
    "        # 1. Forward pass\n",
    "        test_pred_logits = self.forward(X)\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "        test_acc = ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "\n",
    "        self.all_pred = test_pred_labels\n",
    "        self.all_test = y\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "390fef45-0469-4afd-9829-f22f6e9c0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMAttentionModel(pl.LightningModule):\n",
    "    def __init__(self, hidden_size=128, sequence_length=256, input_size=42, cnn_filter_size=64, output_size=10, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.Tensor(1024, sequence_length, input_size)\n",
    "        \n",
    "        self.cnn = nn.Conv1d(sequence_length, cnn_filter_size, kernel_size=5, padding=\"same\")\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.attention1 = SelfAttention(\n",
    "            input_dim=hidden_size)\n",
    "\n",
    "        self.rnn2 = nn.LSTM(input_size=hidden_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.attention2 = SelfAttention(\n",
    "            input_dim=hidden_size\n",
    "        )\n",
    "\n",
    "        self.rnn3 = nn.LSTM(input_size=hidden_size, \n",
    "                  hidden_size=hidden_size,\n",
    "                  num_layers=1,\n",
    "                  batch_first=True)\n",
    "        \n",
    "        self.seq_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seq_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
    "            nn.BatchNorm1d(num_features=hidden_size),\n",
    "            nn.Dropout1d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features=3 * hidden_size, out_features=output_size)\n",
    "        \n",
    "        self.all_test = []\n",
    "        self.all_pred = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.cnn(x)\n",
    "        activation, _ = self.rnn1(output)\n",
    "        activation = self.attention1(activation)\n",
    "        activation, _ = self.rnn2(activation)\n",
    "        activation = self.attention2(activation)\n",
    "        activation, _ = self.rnn3(activation)\n",
    "\n",
    "        b, _, _ = activation.size()\n",
    "        \n",
    "        lstm_output = activation[:,-1,:].view(b,-1)\n",
    "        \n",
    "        seq_1_output = self.seq_1(lstm_output)\n",
    "        seq_2_output = self.seq_2(lstm_output)\n",
    "        \n",
    "        output = torch.concat([lstm_output, seq_1_output, seq_2_output], dim=1)\n",
    "        output = self.classifier(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params=self.parameters(), lr=0.0005)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        # 1. Forward pass\n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        y = y\n",
    "    \n",
    "        # 1. Forward pass\n",
    "        test_pred_logits = self.forward(X)\n",
    "\n",
    "        # Calculate and accumulate accuracy\n",
    "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "        test_acc = ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "        self.log(\"test_acc\", test_acc)\n",
    "        \n",
    "        self.all_pred = test_pred_labels\n",
    "        self.all_test = y\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        X, y = batch\n",
    "        X = X.float()\n",
    "        y = y\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd8d15-df2a-41b0-a1e8-c14122c7efe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dccdd3d4-7679-427e-863a-9a679fb0edc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CNNLSTMModel                             --\n",
       "├─Conv1d: 1-1                            81,984\n",
       "├─LSTM: 1-2                              352,256\n",
       "├─Sequential: 1-3                        --\n",
       "│    └─Linear: 2-1                       16,512\n",
       "│    └─BatchNorm1d: 2-2                  256\n",
       "│    └─Dropout1d: 2-3                    --\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       16,512\n",
       "│    └─BatchNorm1d: 2-6                  256\n",
       "│    └─Dropout1d: 2-7                    --\n",
       "│    └─ReLU: 2-8                         --\n",
       "├─Sequential: 1-4                        --\n",
       "│    └─Linear: 2-9                       16,512\n",
       "│    └─BatchNorm1d: 2-10                 256\n",
       "│    └─Dropout1d: 2-11                   --\n",
       "│    └─ReLU: 2-12                        --\n",
       "│    └─Linear: 2-13                      16,512\n",
       "│    └─BatchNorm1d: 2-14                 256\n",
       "│    └─Dropout1d: 2-15                   --\n",
       "│    └─ReLU: 2-16                        --\n",
       "├─Linear: 1-5                            3,850\n",
       "=================================================================\n",
       "Total params: 505,162\n",
       "Trainable params: 505,162\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNLSTMModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e7bec9-9102-472d-b524-987ab4b0a5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNNLSTMModel'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af66ab22-cfe6-4062-a1b0-2102fad90c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_class = [LSTMModel, LSTMBiModel, LSTMAttentionModel, CNNLSTMModel, CNNLSTMAttentionModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c91c8883-6918-4a9d-a5a9-0ed54a6970d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for test\n",
    "def test():\n",
    "    patience = 20\n",
    "    missing_sensor_numbers = 0 # no missing sensor\n",
    "    user = 2 # use user2 to test\n",
    "    batch_size = 20\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = create_dataset_for_missing_sensor_number(user, missing_sensor_numbers)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=4, # number of subprocesses to use for data loading\n",
    "        shuffle=True)\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=2, # number of subprocesses to use for data loading\n",
    "        shuffle=False)\n",
    "    print(\"next(iter(train_dataloader))[0].shape\", next(iter(train_dataloader))[0].shape)\n",
    "    \n",
    "    for model_class in train_model_class:\n",
    "        \n",
    "        model = model_class(input_size=42, output_size=10)\n",
    "        model_name = model.__class__.__name__\n",
    "        print(\"Running for model\", model_name)\n",
    "\n",
    "        tb_logger = TensorBoardLogger(f\"./loggers/{model_name}\")\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            logger=tb_logger,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=patience, mode=\"min\")],\n",
    "            fast_dev_run = True\n",
    "        )\n",
    "    \n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7fc04-f150-4f23-a0db-d2ef89609792",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "patience = 20\n",
    "# missing_sensor_numbers = 0\n",
    "# all_test_pred = {}\n",
    "start_timer = time.perf_counter()\n",
    "missing_sensor_numbers = 0 ## Changed for no missing sensor\n",
    "\n",
    "for model_class in train_model_class:\n",
    "    for user in [2]: ## Changed for user 2 only\n",
    "        \n",
    "        all_test = []\n",
    "        all_pred = []\n",
    "        \n",
    "        print(f\"\\n*************training on User{user}*************\")\n",
    "        \n",
    "        train_dataset, val_dataset, test_dataset = create_dataset_for_missing_sensor_number(user, missing_sensor_numbers)\n",
    "        \n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size,\n",
    "            num_workers=4, # number of subprocesses to use for data loading\n",
    "            shuffle=True)\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size,\n",
    "            num_workers=2, # number of subprocesses to use for data loading\n",
    "            shuffle=False)\n",
    "\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=2, # number of subprocesses to use for data loading\n",
    "            shuffle=False)\n",
    "        \n",
    "        model = model_class(input_size=42, output_size=10)\n",
    "        model_name = model.__class__.__name__\n",
    "        print(\"Running for model\", model_name)\n",
    "        \n",
    "        tb_logger = TensorBoardLogger(f\"./loggers/{model_name}\")\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            logger=tb_logger,\n",
    "            callbacks=[EarlyStopping(monitor=\"val_loss\", patience=patience, mode=\"min\")],\n",
    "        )\n",
    "    \n",
    "        trainer.fit(model, train_dataloader, val_dataloader)\n",
    "        trainer.test(model, test_dataloader)\n",
    "\n",
    "        all_test.extend(model.all_test)\n",
    "        all_pred.extend(model.all_pred)\n",
    "\n",
    "        test_target_pred = (all_test, all_pred)\n",
    "\n",
    "        os.makedirs(os.path.join(RESULT_FOLDER_PATH, \"98\"), exist_ok=True)\n",
    "        with open(os.path.join(RESULT_FOLDER_PATH, \"98\", f\"Testonly_00_all_test_pred_user_{user}_{missing_sensor_numbers}_{model_name}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(test_target_pred, f)\n",
    "\n",
    "print(\"Executed Time:\", time.perf_counter() - start_timer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
